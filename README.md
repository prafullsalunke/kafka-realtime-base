# Kafka Node Events

A comprehensive Kafka event-streaming example using **Node.js** and **Docker Compose**. This project demonstrates how to build a real-time event streaming system with KafkaJS, featuring both producer and consumer implementations.

## üöÄ Features

- **KafkaJS Integration**: Latest Kafka client for Node.js
- **Docker Compose Setup**: Complete Kafka + Zookeeper + UI environment
- **Producer with Retry Logic**: Robust event publishing with exponential backoff
- **Consumer with Graceful Handling**: Continuous message consumption with error recovery
- **Kafdrop UI**: Web-based Kafka management interface
- **Sample Events**: Realistic e-commerce event examples
- **Environment Configuration**: Flexible configuration via environment variables

## üìã Prerequisites

- **Node.js** 18+ (LTS recommended)
- **Docker** and **Docker Compose**
- **Git**

## üõ†Ô∏è Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd kafka-node-events
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Set up environment variables**
   ```bash
   cp env.example .env
   # Edit .env file if needed (defaults work for local development)
   ```

## üê≥ Quick Start

### Option 1: Full Demo (Recommended)
```bash
# Start everything and run the complete demo
npm run setup
```

### Option 2: Step by Step
```bash
# 1. Start Kafka infrastructure
npm run docker:up

# 2. Wait for services to be ready (check logs)
npm run docker:logs

# 3. Run the complete demo
npm start
```

## üìÅ Project Structure

```
kafka-node-events/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml          # Kafka & Syslog-ng setup
‚îú‚îÄ‚îÄ env.example                 # Environment configuration template
‚îú‚îÄ‚îÄ package.json               # Node.js dependencies and scripts
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ syslog-ng.conf         # Syslog-ng configuration
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ view-logs.sh           # Log viewer script
‚îú‚îÄ‚îÄ logs/                      # üìç LOCAL LOG STORAGE (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ kafka/                 # Kafka data and logs
‚îÇ   ‚îú‚îÄ‚îÄ kafka-logs/            # Kafka application logs
‚îÇ   ‚îú‚îÄ‚îÄ kafka-ui/              # Kafka UI logs
‚îÇ   ‚îî‚îÄ‚îÄ syslog-ng/             # Syslog-ng centralized logs
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kafka.js           # Kafka client configuration
‚îÇ   ‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ produce.js         # Event publishing logic
‚îÇ   ‚îú‚îÄ‚îÄ consumer/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ consume.js         # Message consumption logic
‚îÇ   ‚îî‚îÄ‚îÄ index.js               # Main demo entry point
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üéØ Usage

### Available Scripts

| Command | Description |
|---------|-------------|
| `npm start` | Run the complete demo (producer + consumer) |
| `npm run produce` | Publish sample events to Kafka |
| `npm run consume` | Start consuming messages from Kafka |
| `npm run dev` | Run with nodemon for development |
| `npm run docker:up` | Start Kafka infrastructure |
| `npm run docker:down` | Stop Kafka infrastructure |
| `npm run docker:logs` | View Docker logs |
| `npm run setup` | Complete setup and demo |

#### Log Management Commands

| Command | Description |
|---------|-------------|
| `npm run logs:kafka` | View Kafka broker logs |
| `npm run logs:ui` | View Kafka UI logs |
| `npm run logs:syslog` | View syslog-ng logs |
| `npm run logs:events` | View event messages from Kafka |
| `npm run logs:all` | Monitor all logs in real-time |
| `npm run logs:clean` | Clean all log files |

### Individual Components

#### Producer
```bash
npm run produce
```
Publishes 5 sample events:
- `user_signup`
- `order_created`
- `payment_processed`
- `product_viewed`
- `cart_updated`

#### Consumer
```bash
npm run consume
```
Continuously consumes messages with:
- Real-time processing
- Partition and offset tracking
- Simulated processing delays
- Graceful error handling

## üîß Configuration

### Environment Variables

Create a `.env` file based on `env.example`:

```env
# Kafka Configuration
KAFKA_BROKER_URL=localhost:9092
KAFKA_TOPIC=events-log
KAFKA_CLIENT_ID=kafka-node-events
KAFKA_CONSUMER_GROUP_ID=events-consumer-group

# Producer Configuration
PRODUCER_RETRY_ATTEMPTS=3

# Consumer Configuration
CONSUMER_SESSION_TIMEOUT=30000
```

### Docker Services

| Service | Port | Description |
|---------|------|-------------|
| Kafka | 9092 | Main Kafka broker (KRaft mode) |
| Kafdrop UI | 9000 | Web-based Kafka management |
| Syslog-ng | 514 | Centralized logging system |

## üìä Monitoring & Logging

### Kafdrop UI
Access the Kafka management interface at: http://localhost:9000

Features:
- Topic management
- Message browsing
- Consumer group monitoring
- Real-time metrics

### Local Log Storage
All logs are stored locally in the `./logs/` directory:

```bash
./logs/
‚îú‚îÄ‚îÄ kafka/              # Kafka data & logs
‚îú‚îÄ‚îÄ kafka-logs/         # Kafka application logs  
‚îú‚îÄ‚îÄ kafka-ui/           # Kafka UI logs
‚îî‚îÄ‚îÄ syslog-ng/          # Syslog-ng centralized logs
```

### Log Viewing Commands
```bash
# View different log types
npm run logs:kafka      # Kafka broker logs
npm run logs:ui         # Kafka UI logs
npm run logs:syslog     # Syslog-ng logs
npm run logs:events     # Event messages from Kafka
npm run logs:all        # Monitor all logs in real-time
npm run logs:clean      # Clean all log files
```

### Docker Logs
```bash
# View all logs
npm run docker:logs

# View specific service logs
docker-compose logs -f kafka
docker-compose logs -f syslog-ng
```

## üß™ Sample Events

The producer generates realistic e-commerce events:

```json
{
  "type": "user_signup",
  "userId": "user_123",
  "email": "user123@example.com",
  "timestamp": "2024-01-15T10:30:00.000Z",
  "metadata": {
    "source": "web",
    "ip": "192.168.1.100"
  }
}
```

## üîÑ Event Flow

1. **Producer** connects to Kafka broker
2. **Events** are published to `events-log` topic
3. **Consumer** subscribes to the topic
4. **Messages** are processed with metadata logging
5. **Processing delays** simulate real-world scenarios

## üõ°Ô∏è Error Handling

### Producer
- Automatic retry with exponential backoff
- Connection error recovery
- Graceful shutdown

### Consumer
- Message parsing error handling
- Reconnection logic
- Graceful shutdown on SIGINT/SIGTERM

## üöÄ Production Considerations

### Scaling
- Multiple consumer instances for load balancing
- Partition-based parallelism
- Consumer group coordination

### Monitoring & Logging
- Kafka metrics and health checks
- Application logging and alerting
- Performance monitoring
- Centralized logging with syslog-ng
- Local log storage for easy access

### Security
- SASL/SSL authentication
- Topic access control
- Network security

## üêõ Troubleshooting

### Common Issues

1. **Kafka not starting**
   ```bash
   # Check Docker resources
   docker system prune
   docker-compose down -v
   docker-compose up -d
   ```

2. **Connection refused**
   ```bash
   # Wait for services to be ready
   npm run docker:logs
   # Check if port 9092 is available
   ```

3. **Consumer not receiving messages**
   ```bash
   # Check consumer group
   # Verify topic exists
   # Check offset configuration
   ```

4. **Log viewing issues**
   ```bash
   # Check if logs directory exists
   ls -la logs/
   
   # Clean and restart if needed
   npm run logs:clean
   docker-compose restart
   ```

### Debug Mode
```bash
# Enable debug logging
LOG_LEVEL=debug npm start

# View all logs in real-time
npm run logs:all
```

## üìö Resources

- [KafkaJS Documentation](https://kafka.js.org/)
- [Confluent Kafka Docker](https://docs.confluent.io/platform/current/quickstart/docker.html)
- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)
- [Syslog-ng Documentation](https://www.syslog-ng.com/technical-documents/)

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## üìÑ License

MIT License - see LICENSE file for details

---

**Happy Event Streaming! üéâ** 

## Running Producer and Consumer via Docker Compose

To run the producer:

```sh
docker-compose run --rm producer
```

To run the consumer:

```sh
docker-compose run --rm consumer
```

Both services use the same code and .env file, and will connect to the Kafka broker at `kafka:9092`.

> **Note:** If you run the Node.js scripts on your host, set `KAFKA_BROKER_URL=localhost:9092` in your `.env`. For Docker Compose, use `kafka:9092`. 